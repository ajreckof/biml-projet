{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 : Perceptron\n",
    "\n",
    "**Indiquer et expliquer la taille de chaque tenseur dans le fichier perceptron pytorch.py fourni.**\n",
    "\n",
    "- $w$ est de taille $784 \\times 10$. $784$ est le nombre de pixels dans chaque image ($28 \\times 28$) et donc la taille de l'entrée et $10$ est la taille des vecteurs de label et de la sortie.  \n",
    "- $b$ est de taille $1 \\times 10$ car c'est le biais qui est de même dimension que la sortie.  \n",
    "- $data\\_train$ est de taille $784 \\times nb\\_data\\_train$, $nb\\_data\\_train$ est le nombre d'images dans le set d'entrainement.  \n",
    "- $data\\_test$ est de taille $784 \\times nb\\_data\\_test$, $nb\\_data\\_test$ est le nombre d'images dans le set de test.  \n",
    "- $x$ est un batch de $data\\_train$ ($784 \\times batch\\_size$)\n",
    "- $y$ est la sortie associé au batch $x$ ($10 \\times batch\\_size$)\n",
    "- $t$ est un batch des labels, associés à $x$ ($10 \\times batch\\_size$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 : Shallow network\n",
    "\n",
    "Implémentation de l’algorithme du perceptron multi-couches avec une seule couche cachée et une sortie linéaire.  \n",
    "L'objectif est de trouver les hyperparamètres $\\eta$ et le nombre de neurones de la couche cachée ($N$).\n",
    "\n",
    "### Méthodologie\n",
    "\n",
    "Le $\\eta$ par défaut nous paraissait bien trop petit, nous avons observé que même après 10 epochs le perceptron continuait d'apprendre. Nous avons augmenté sa valeur à : $\\eta = 0.001$. Cette valeur nous aprait correcte car l'accuracy de notre modèle se stabilise autours des 96% pour un $N = 100$.  \n",
    "\n",
    "### Influence des paramètres sur la performance\n",
    "\n",
    "$\\eta$ à une influence sur la vitesse d'apprentissage. Si on se limite à 10 epochs l'apprentissage doit être assez rapide pour obtenir des résultats satisfaisants.\n",
    "\n",
    "$N$ influence la précision du résultat, plus il y a de neurones plus la probabilité de trouver le bon résultat augmente mais aussi plus l'apprentissage est lent. En effet, plus de neurones signifie plus de calculs donc plus de temps d'apprentissage.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
