{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet BIML\n",
    "\n",
    "Bonhoure Timothé 11931551 et Martinez Christophe 11709105"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 : Perceptron\n",
    "\n",
    "**Indiquer et expliquer la taille de chaque tenseur dans le fichier perceptron pytorch.py fourni.**\n",
    "\n",
    "- $w$ est de taille $784 \\times 10$. $784$ est le nombre de pixels dans chaque image ($28 \\times 28$) et donc la taille de l'entrée et $10$ est la taille des vecteurs de label et de la sortie.  \n",
    "- $b$ est de taille $1 \\times 10$ car c'est le biais qui est de même dimension que la sortie.  \n",
    "- $data\\_train$ est de taille $784 \\times nb\\_data\\_train$, $nb\\_data\\_train$ est le nombre d'images dans le set d'entrainement.  \n",
    "- $data\\_test$ est de taille $784 \\times nb\\_data\\_test$, $nb\\_data\\_test$ est le nombre d'images dans le set de test.  \n",
    "- $x$ est un batch de $data\\_train$ ($784 \\times batch\\_size$)\n",
    "- $y$ est la sortie associé au batch $x$ ($10 \\times batch\\_size$)\n",
    "- $t$ est un batch des labels, associés à $x$ ($10 \\times batch\\_size$)\n",
    "- $grad$ est le tenseur des gradient et est de même dimensions que $t$ et $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 : Shallow network\n",
    "\n",
    "Implémentation de l’algorithme du perceptron multi-couches avec une seule couche cachée et une sortie linéaire.  \n",
    "L'objectif est de trouver les hyperparamètres $\\eta$ et le nombre de neurones de la couche cachée ($N$).\n",
    "\n",
    "### Méthodologie\n",
    "\n",
    "```python\n",
    "class ShallowNetwork(nn.Module):\n",
    "\tdef __init__(self, N) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.linear1 = nn.Linear(784, N)\n",
    "\t\tnn.init.uniform_(self.linear1.weight, -0.001, 0.001)\n",
    "\t\tself.linear2 = nn.Linear(N, 10)\n",
    "\t\tnn.init.uniform_(self.linear2.weight, -0.001, 0.001)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.linear1(x)\n",
    "\t\tx = F.relu(x)\n",
    "\t\treturn self.linear2(x)\n",
    "```\n",
    "\n",
    "Le $\\eta$ par défaut nous paraissait bien trop petit, nous avons observé que même après 10 epochs le perceptron continuait d'apprendre. Nous avons augmenté sa valeur à : $\\eta = 0.001$. Cette valeur nous aprait correcte car l'accuracy de notre modèle se stabilise autours des 96% pour un $N = 100$.  \n",
    "\n",
    "Pour l'hyperparamètre $N$ nous avons essayé des valeurs un peu extrêmes telle que $N=10$ ou $N=1000$.  \n",
    "Pour $N=10$, l'apprentissage était rapide mais l'accuracy était plutôt mauvaise ($\\approx 0.60$).  \n",
    "Pour $N=1000$, l'apprentissage est lent mais l'accuracy monte à $\\approx 0.98$.\n",
    "On observe que pour $N=100$, l'accuracy reste largement correct ($\\approx 0.95$) et le temps d'exécution est $\\approx 20 sec$.\n",
    "\n",
    "### Influence des paramètres sur la performance\n",
    "\n",
    "$\\eta$ à une influence sur la vitesse d'apprentissage. Si on se limite à 10 epochs l'apprentissage doit être assez rapide pour se stabiliser et obtenir des résultats satisfaisants.\n",
    "\n",
    "$N$ influence la précision du résultat, plus il y a de neurones plus la probabilité de trouver le bon résultat augmente mais aussi plus l'apprentissage est lent. En effet, plus de neurones signifie plus de calculs donc plus de temps d'apprentissage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 3 : Deep network\n",
    "\n",
    "On cherche à déterminer les hyperparamètres $\\eta$, le nombre de couches ($N_C$), le nombre de neurones pour chaque couche ($N_i$ avec $i \\in [\\![1,N_C]\\!]$) et la taille des batch ($batch\\_size$).\n",
    "\n",
    "### Méthodologie\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "Nous avons décidé de limiter les choix des hyperparamètres :\n",
    "- $\\eta \\in \\{0.01, 0.001, 0.0001\\}$ \n",
    "- $N_C \\in \\{2,3,4\\}$\n",
    "- $N_i \\in \\{10, 50, 100\\}$\n",
    "- $batch\\_size \\in \\{5,10,15\\}$\n",
    "\n",
    "Ces limites ont été réfléchi en prenant en compte une limitation en puissance de calcul et en temps."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
