{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet BIML\n",
    "\n",
    "Bonhoure Timothé 11931551 et Martinez Christophe 11709105"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 : Perceptron\n",
    "\n",
    "**Indiquer et expliquer la taille de chaque tenseur dans le fichier perceptron pytorch.py fourni.**\n",
    "\n",
    "- $w$ est de taille $784 \\times 10$. $784$ est le nombre de pixels dans chaque image ($28 \\times 28$) et donc la taille de l'entrée et $10$ est la taille des vecteurs de label et de la sortie.  \n",
    "- $b$ est de taille $1 \\times 10$ car c'est le biais qui est de même dimension que la sortie.  \n",
    "- $data\\_train$ est de taille $784 \\times nb\\_data\\_train$, $nb\\_data\\_train$ est le nombre d'images dans le set d'entrainement.  \n",
    "- $data\\_test$ est de taille $784 \\times nb\\_data\\_test$, $nb\\_data\\_test$ est le nombre d'images dans le set de test.  \n",
    "- $x$ est un batch de $data\\_train$ ($784 \\times batch\\_size$)\n",
    "- $y$ est la sortie associé au batch $x$ ($10 \\times batch\\_size$)\n",
    "- $t$ est un batch des labels, associés à $x$ ($10 \\times batch\\_size$)\n",
    "- $grad$ est le tenseur des gradient et est de même dimensions que $t$ et $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 : Shallow network\n",
    "\n",
    "Implémentation de l’algorithme du perceptron multi-couches avec une seule couche cachée et une sortie linéaire.  \n",
    "L'objectif est de trouver les hyperparamètres $\\eta$ et le nombre de neurones de la couche cachée ($N$).\n",
    "\n",
    "### Méthodologie\n",
    "\n",
    "```python\n",
    "class ShallowNetwork(nn.Module):\n",
    "\tdef __init__(self, N) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.linear1 = nn.Linear(784, N)\n",
    "\t\tnn.init.uniform_(self.linear1.weight, -0.001, 0.001)\n",
    "\t\tself.linear2 = nn.Linear(N, 10)\n",
    "\t\tnn.init.uniform_(self.linear2.weight, -0.001, 0.001)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.linear1(x)\n",
    "\t\tx = F.relu(x)\n",
    "\t\treturn self.linear2(x)\n",
    "```\n",
    "\n",
    "Le $\\eta$ par défaut nous paraissait bien trop petit, nous avons observé que même après 10 epochs le perceptron continuait d'apprendre. Nous avons augmenté sa valeur à : $\\eta = 0.001$. Cette valeur nous aprait correcte car l'accuracy de notre modèle se stabilise autours des 96% pour un $N = 100$.  \n",
    "\n",
    "Pour l'hyperparamètre $N$ nous avons essayé des valeurs un peu extrêmes telle que $N=10$ ou $N=1000$.  \n",
    "Pour $N=10$, l'apprentissage était rapide mais l'accuracy était plutôt mauvaise ($\\approx 0.60$).  \n",
    "Pour $N=1000$, l'apprentissage est lent mais l'accuracy monte à $\\approx 0.98$.\n",
    "On observe que pour $N=100$, l'accuracy reste largement correct ($\\approx 0.95$) et le temps d'exécution est $\\approx 20 sec$.\n",
    "\n",
    "### Influence des paramètres sur la performance\n",
    "\n",
    "$\\eta$ à une influence sur la vitesse d'apprentissage. Si on se limite à 10 epochs l'apprentissage doit être assez rapide pour se stabiliser et obtenir des résultats satisfaisants.\n",
    "\n",
    "$N$ influence la précision du résultat, plus il y a de neurones plus la probabilité de trouver le bon résultat augmente mais aussi plus l'apprentissage est lent. En effet, plus de neurones signifie plus de calculs donc plus de temps d'apprentissage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 3 : Deep network\n",
    "\n",
    "On cherche à déterminer les hyperparamètres $\\eta$, le nombre de couches ($N_C$), le nombre de neurones pour chaque couche ($N_i$ avec $i \\in [\\![1,N_C]\\!]$) et la taille des batch ($batch\\_size$).\n",
    "\n",
    "### Méthodologie\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "Nous avons décidé de limiter les choix des hyperparamètres :\n",
    "- $\\eta \\in \\{0.01, 0.001, 0.0001\\}$ \n",
    "- $N_C \\in \\{2,3,4\\}$\n",
    "- $N_i \\in \\{10, 50, 100\\}$\n",
    "- $batch\\_size \\in \\{5,10,15\\}$\n",
    "\n",
    "Ces limites ont été réfléchi en prenant en compte une limitation en puissance de calcul et en temps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 4 : CNN\n",
    "\n",
    "L'objectif est d'implémenter un réseau de neurones convolutif.\n",
    "\n",
    "### Méthodologie\n",
    "\n",
    "```python\n",
    "class LeNet5(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.layer1 = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2),\n",
    "\t\t\tnn.BatchNorm2d(6),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "\t\tself.layer2 = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "\t\t\tnn.BatchNorm2d(16),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "\t\tself.linear1 = nn.Linear(400, 120)\n",
    "\t\tnn.init.uniform_(self.linear1.weight, -0.001, 0.001)\n",
    "\t\tself.linear2 = nn.Linear(120, 84)\n",
    "\t\tnn.init.uniform_(self.linear2.weight, -0.001, 0.001)\n",
    "\t\tself.linear3 = nn.Linear(84, 10)\n",
    "\t\tnn.init.uniform_(self.linear3.weight, -0.001, 0.001)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = x.view(x.size(0),1,28,28)\n",
    "\t\tx = self.layer1(x)\n",
    "\t\tx = self.layer2(x)\n",
    "\t\tx = x.reshape(x.size(0), -1)\n",
    "\t\tx = self.linear1(x)\n",
    "\t\tx = F.relu(x)\n",
    "\t\tx = self.linear2(x)\n",
    "\t\tx = F.relu(x)\n",
    "\t\tx = self.linear3(x)\n",
    "\t\treturn x\n",
    "```\n",
    "\n",
    "L'architecture du réseau est inspirée de celle donnée dans [Lecun](https://hal.science/hal-03926082/document).  \n",
    "La `layer1` exécute la 1ère convolution(`nn.Conv2d`), elle prend en entré un tensor de dimension $(batch\\_size, 1, 28, 28)$, le kernel étant de taille 5, cette convolution nécessitait un padding de 2 pour conserver en sortie des matrices de dimension $(28,28)$. Ensuite, `nn.MaxPool2d` applique le subsampling de matrices de $(28,28)$ à des matrices de $(14,14)$.  \n",
    "La `layer2` exécute la 2ème convolution prenant les matrices précédentes pour sortir 16 matrices de $(10,10)$. Le subsampling suivant transforme les matrices en dimension $(5,5)$.  \n",
    "Ensuite, des couches similaires au perceptron multi-couches permettent de réduire les résultats en un vecteur de 10 valeurs en faisant succéder une couche linéaire, d'une relu, linéaire, relu et enfin linéaire.\n",
    "\n",
    "Résultats (à gauche l'accuracy sur le batch de test et à droite le temps d'exécution accumulé en secondes) :\n",
    "> Sur mon ordinateur\n",
    "```python\n",
    "tensor([0.1996]) 50.06423878669739\n",
    "tensor([0.9476]) 98.38135719299316\n",
    "tensor([0.9780]) 149.69705057144165\n",
    "tensor([0.9816]) 215.07379031181335\n",
    "tensor([0.9851]) 268.4216022491455\n",
    "tensor([0.9834]) 325.4106557369232\n",
    "tensor([0.9863]) 376.84096574783325\n",
    "tensor([0.9850]) 429.86590003967285\n",
    "tensor([0.9847]) 481.203097820282\n",
    "tensor([0.9841]) 532.2355885505676\n",
    "```\n",
    "> Sur kaggle\n",
    "```python\n",
    "tensor([0.4809]) 48.37343430519104\n",
    "tensor([0.9669]) 94.88335609436035\n",
    "tensor([0.9789]) 140.88308835029602\n",
    "tensor([0.9854]) 187.28817486763\n",
    "tensor([0.9830]) 232.813392162323\n",
    "tensor([0.9854]) 278.8786988258362\n",
    "tensor([0.9873]) 324.19943380355835\n",
    "tensor([0.9873]) 369.5354652404785\n",
    "tensor([0.9867]) 415.27380990982056\n",
    "tensor([0.9857]) 461.0540907382965\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO a effacer  \n",
    "> DeepNetwork :\n",
    "\n",
    "|$learning\\_rate$|0.0001|||0.001|||0.01|||\n",
    "|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|$batch\\_size$|5|10|15|5|10|15|5|10|15|\n",
    "||||||||"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
