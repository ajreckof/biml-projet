{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet BIML\n",
    "\n",
    "Bonhoure Timothé 11931551 et Martinez Christophe 11709105"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1 : Perceptron\n",
    "\n",
    "**Indiquer et expliquer la taille de chaque tenseur dans le fichier perceptron pytorch.py fourni.**\n",
    "\n",
    "- $w$ est de taille $784 \\times 10$. $784$ est le nombre de pixels dans chaque image ($28 \\times 28$) et donc la taille de l'entrée et $10$ est la taille des vecteurs de label et de la sortie.  \n",
    "- $b$ est de taille $1 \\times 10$ car c'est le biais qui est de même dimension que la sortie.  \n",
    "- $data\\_train$ est de taille $784 \\times nb\\_data\\_train$, $nb\\_data\\_train$ est le nombre d'images dans le set d'entrainement.  \n",
    "- $data\\_test$ est de taille $784 \\times nb\\_data\\_test$, $nb\\_data\\_test$ est le nombre d'images dans le set de test.  \n",
    "- $x$ est un batch de $data\\_train$ ($784 \\times batch\\_size$)\n",
    "- $y$ est la sortie associé au batch $x$ ($10 \\times batch\\_size$)\n",
    "- $t$ est un batch des labels, associés à $x$ ($10 \\times batch\\_size$)\n",
    "- $grad$ est le tenseur des gradient et est de même dimensions que $t$ et $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 : Shallow network\n",
    "\n",
    "Implémentation de l’algorithme du perceptron multi-couches avec une seule couche cachée et une sortie linéaire.  \n",
    "L'objectif est de trouver les hyper-paramètres $\\eta$ et le nombre de neurones de la couche cachée ($N$).\n",
    "\n",
    "### Méthodologie\n",
    "\n",
    "```python\n",
    "class ShallowNetwork(nn.Module):\n",
    "\tdef __init__(self, N) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.linear1 = nn.Linear(784, N)\n",
    "\t\tnn.init.uniform_(self.linear1.weight, -0.001, 0.001)\n",
    "\t\tself.linear2 = nn.Linear(N, 10)\n",
    "\t\tnn.init.uniform_(self.linear2.weight, -0.001, 0.001)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.linear1(x)\n",
    "\t\tx = F.relu(x)\n",
    "\t\treturn self.linear2(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lire les données CSV\n",
    "data = pd.read_csv('results_shallow.csv')\n",
    "\n",
    "# Séparer les données en fonction du taux d'apprentissage\n",
    "learning_rates = data['learning_rate'].unique()\n",
    "\n",
    "# Créer une figure avec deux sous-graphiques\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Parcourir les taux d'apprentissage et créer les graphiques\n",
    "for lr in learning_rates:\n",
    "    subset = data[data['learning_rate'] == lr]\n",
    "    ax1.plot(subset['number_of_neurones'], 1-subset['result'], label=f'LR {lr}')\n",
    "    ax2.plot(subset['number_of_neurones'], subset['time'], label=f'LR {lr}')\n",
    "\n",
    "# Configurer le premier graphique (Résultats)\n",
    "ax1.set_xlabel('Nombre de neurones')\n",
    "ax1.set_ylabel('Taux d\\'erreur')\n",
    "ax1.set_yscale(\"log\")\n",
    "yticks = [0.01 * i for i in range(1, 11)] + [0.1 * i for i in range(1, 5)]\n",
    "ax1.set_yticks(yticks)\n",
    "ax1.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "ax1.set_title('Taux d\\'erreur en fonction du nombre de neurones')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Configurer le deuxième graphique (Temps d'exécution)\n",
    "ax2.set_xlabel('Nombre de neurones')\n",
    "ax2.set_ylabel('Temps d\\'exécution (s)')\n",
    "ax2.set_title('Temps d\\'exécution en fonction du nombre de neurones')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# Ajuster l'espacement entre les sous-graphiques\n",
    "plt.tight_layout()\n",
    "\n",
    "# Afficher les graphiques\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![shallow_results.png](img/shallow_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence des hyperparamètres\n",
    "\n",
    "On observe que le nombre de neurones n'a plus d'effet sur le pourcentage de réussite du modèle à partir de 300 à 500 neurones. En revanche, le learning_rate est le paramètre qui a le plus d'effet globalement.  \n",
    "En terme de temps d'exécution, on observe un pic que l'on ne saurait expliquer pour un nombre de neurones de 100. En dehors de cela, le nombre de neurones à une grande influence sur le temps d'exécution et ceci est cohérent car plus de neurones signifie plus de nombre de modification des poids. Les différents learning_rate sont globalement proche en terme de temps d'exécution mais diffèrent pour un nombre de neurones autours de 200.\n",
    "\n",
    "Sur 1000 epochs, on voit d'après la courbe ci-dessus à gauche le meilleur résultat est obtenu avec un learning_rate est de 0.01 et pour un nombre de neurone de minimum 500. Or si on s'intéresse au temps d'apprentissage, pour un résultat assez proche et convenable : un learning_rate de 0.001 avec un nombre de neurones entre 200 et 500 est l'optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 3 : Deep network\n",
    "\n",
    "On cherche à déterminer les hyperparamètres $\\eta$, le nombre de couches ($N_C$), le nombre de neurones pour chaque couche ($N_i$ avec $i \\in [\\![1,N_C]\\!]$) et la taille des batch ($batch\\_size$).\n",
    "\n",
    "### Méthodologie\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "Nous avons décidé de limiter les choix des hyperparamètres :\n",
    "- $\\eta \\in \\{0.01, 0.001, 0.0001\\}$ \n",
    "- $N_C \\in \\{2,3,4\\}$\n",
    "- $N_i \\in \\{10, 50, 100\\}$\n",
    "- $batch\\_size \\in \\{5,10,15\\}$\n",
    "\n",
    "Ces limites ont été réfléchi en prenant en compte une limitation en puissance de calcul et en temps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "résultat grid search incrementing_factor : `0.9871428608894348`\n",
    "\n",
    "résultat grid_search first_layer_size :`0.9872857332229614`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 4 : CNN\n",
    "\n",
    "L'objectif est d'implémenter un réseau de neurones convolutif.\n",
    "\n",
    "### Méthodologie\n",
    "\n",
    "```python\n",
    "class LeNet5(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.layer1 = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2),\n",
    "\t\t\tnn.BatchNorm2d(6),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "\t\tself.layer2 = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "\t\t\tnn.BatchNorm2d(16),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "\t\tself.linear1 = nn.Linear(400, 120)\n",
    "\t\tnn.init.uniform_(self.linear1.weight, -0.001, 0.001)\n",
    "\t\tself.linear2 = nn.Linear(120, 84)\n",
    "\t\tnn.init.uniform_(self.linear2.weight, -0.001, 0.001)\n",
    "\t\tself.linear3 = nn.Linear(84, 10)\n",
    "\t\tnn.init.uniform_(self.linear3.weight, -0.001, 0.001)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = x.view(x.size(0),1,28,28)\n",
    "\t\tx = self.layer1(x)\n",
    "\t\tx = self.layer2(x)\n",
    "\t\tx = x.reshape(x.size(0), -1)\n",
    "\t\tx = self.linear1(x)\n",
    "\t\tx = F.relu(x)\n",
    "\t\tx = self.linear2(x)\n",
    "\t\tx = F.relu(x)\n",
    "\t\tx = self.linear3(x)\n",
    "\t\treturn x\n",
    "```\n",
    "\n",
    "L'architecture du réseau est inspirée de celle donnée dans [Lecun](https://hal.science/hal-03926082/document).  \n",
    "La `layer1` exécute la 1ère convolution(`nn.Conv2d`), elle prend en entré un tensor de dimension $(batch\\_size, 1, 28, 28)$, le kernel étant de taille 5, cette convolution nécessitait un padding de 2 pour conserver en sortie des matrices de dimension $(28,28)$. Ensuite, `nn.MaxPool2d` applique le subsampling de matrices de $(28,28)$ à des matrices de $(14,14)$.  \n",
    "La `layer2` exécute la 2ème convolution prenant les matrices précédentes pour sortir 16 matrices de $(10,10)$. Le subsampling suivant transforme les matrices en dimension $(5,5)$.  \n",
    "Ensuite, des couches similaires au perceptron multi-couches permettent de réduire les résultats en un vecteur de 10 valeurs en faisant succéder une couche linéaire, d'une relu, linéaire, relu et enfin linéaire.\n",
    "\n",
    "Résultats (à gauche l'accuracy sur le batch de test et à droite le temps d'exécution accumulé en secondes) :\n",
    "> Sur mon ordinateur\n",
    "```python\n",
    "tensor([0.1996]) 50.06423878669739\n",
    "tensor([0.9476]) 98.38135719299316\n",
    "tensor([0.9780]) 149.69705057144165\n",
    "tensor([0.9816]) 215.07379031181335\n",
    "tensor([0.9851]) 268.4216022491455\n",
    "tensor([0.9834]) 325.4106557369232\n",
    "tensor([0.9863]) 376.84096574783325\n",
    "tensor([0.9850]) 429.86590003967285\n",
    "tensor([0.9847]) 481.203097820282\n",
    "tensor([0.9841]) 532.2355885505676\n",
    "```\n",
    "> Sur kaggle\n",
    "```python\n",
    "tensor([0.4809]) 48.37343430519104\n",
    "tensor([0.9669]) 94.88335609436035\n",
    "tensor([0.9789]) 140.88308835029602\n",
    "tensor([0.9854]) 187.28817486763\n",
    "tensor([0.9830]) 232.813392162323\n",
    "tensor([0.9854]) 278.8786988258362\n",
    "tensor([0.9873]) 324.19943380355835\n",
    "tensor([0.9873]) 369.5354652404785\n",
    "tensor([0.9867]) 415.27380990982056\n",
    "tensor([0.9857]) 461.0540907382965\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO a effacer  \n",
    "> DeepNetwork :\n",
    "\n",
    "|$learning\\_rate$|0.0001|||0.001|||0.01|||\n",
    "|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|$batch\\_size$|5|10|15|5|10|15|5|10|15|\n",
    "||||||||"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5a318b75a7586e7126ee807ac3c15c9d32b27634cac574e72ad2f13c9c76b5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
